Business Use Case: Telecommunication Data Analytics with Hive Partitioning and Bucketing
Problem Statement
A telecommunication company processes vast amounts of data related to customer calls, SMS usage, and data consumption. This data is used to analyze telecom usage trends, such as:
1. Call Durations by region and date.
2. Top Data Users in specific regions.
3. SMS usage trends based on customer demographics.

Given the large scale of the dataset (millions of records), querying the data for business insights is slow without proper optimization. The company wants to:
- Optimize queries to retrieve data based on call date and region for reporting purposes.
- Speed up queries to identify heavy data users.
- Use partitioning and bucketing to manage the dataset efficiently in Hive, improving query performance and storage optimization.
Sample Data
Call Data Table (call_data)
Columns:
- call_id: INT
- customer_id: INT
- call_duration: FLOAT
- region: STRING
- call_date: DATE

Sample Data:
1, 101, 15.5, North, 2023-08-01
2, 102, 20.2, South, 2023-08-02
3, 103, 5.7, East, 2023-08-03
4, 104, 12.4, West, 2023-08-04
5, 105, 25.0, North, 2023-08-05
Data Usage Table (data_usage)
Columns:
- usage_id: INT
- customer_id: INT
- data_used: FLOAT (in GB)
- region: STRING
- usage_date: DATE

Sample Data:
1, 101, 2.5, North, 2023-08-01
2, 102, 3.0, South, 2023-08-02
3, 103, 1.2, East, 2023-08-03
4, 104, 5.5, West, 2023-08-04
5, 105, 10.0, North, 2023-08-05
SMS Data Table (sms_data)
Columns:
- sms_id: INT
- customer_id: INT
- sms_count: INT
- region: STRING
- sms_date: DATE

Sample Data:
1, 101, 5, North, 2023-08-01
2, 102, 10, South, 2023-08-02
3, 103, 8, East, 2023-08-03
4, 104, 7, West, 2023-08-04
5, 105, 15, North, 2023-08-05

Solution: Using Partitioning and Bucketing in Hive

1. Create data file in local and upload it in HDFS:

First three directories were created in local machine and three files call_data.csv, data_usage.csv and sms_data.csv were created inside 
the directory. Sample data was then copied, pasted and saved using nano command in those files.
Now three different directories was created in HDFS inside a parent directory telecom into which csv files were uploaded from the local
machine. Now we are ready for tables creation in Hive, data loading from HDFS into the table 
and running queries. The following command was used to upload file from local machine to HDFS.

hadoop fs -put call_data.csv /data/test/telecom/call_data
hadoop fs -put data_usage.csv /data/test/telecom/data_usage
hadoop fs -put sms_data.csv /data/test/telecom/sms_data

2. Partitioning
Partition the data by call_date and region. This helps reduce the amount of data scanned when querying for specific dates or regions.
- Partition on: region, call_date

3. Bucketing
Use bucketing on customer_id for all tables (call_data, data_usage, sms_data). This allows for efficient queries targeting specific customers.
- Bucket on: customer_id

4. Create a non partitioned external table:

Hive Query to create the table without any partitioning and bucketing:

CREATE external TABLE non_partition_sales_data (
sale_id INT,
product_id INT,
product_category STRING,
customer_id INT,
sale_amount FLOAT,
sale_date STRING,
country STRING,
region STRING
) 
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' 
LOCATION '/data/test/ecommerce/sales_data';

5. Create Table with Partitioning and Bucketing:

Hive Query to create the table with partitioning and bucketing:

CREATE TABLE sales_data (
sale_id INT,
product_id INT,
product_category STRING,
customer_id INT,
sale_amount FLOAT,
region STRING
)
PARTITIONED BY (sale_date STRING, country STRING)
CLUSTERED BY (product_category, customer_id) INTO 10 BUCKETS
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
STORED AS ORC;
